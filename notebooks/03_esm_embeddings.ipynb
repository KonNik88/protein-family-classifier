{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02077c52-91de-4789-bb0d-aecacdb65b69",
   "metadata": {},
   "source": [
    "# 03 — ESM-1b Embedding Generation\r\n",
    "\r\n",
    "In this notebook we compute high-dimensional protein embeddings using  \r\n",
    "**ESM-1b (esm1b_t33_650M_UR50S)** — a state-of-the-art transformer model for proteins.\r\n",
    "\r\n",
    "We use the cleaned dataset \n",
    "**data/processed/protein_families_small_clean.csv**\n",
    "\n",
    "\r\n",
    "### Goals of this notebook\r\n",
    "\r\n",
    "1. **Load the cleaned dataset** and extract FASTA-like sequences.\r\n",
    "2. **Load ESM-1b model and alphabet** using the `fair-esm` library.\r\n",
    "3. **Tokenize sequences and compute embeddings**:\r\n",
    "   - Representation layer: **33**\r\n",
    "   - Mean pooling over amino acids (excluding BOS/EOS and padding)\r\n",
    "   - Sequence length limit enforced at 1000 AA (ESM-1b max context)\r\n",
    "4. **Batch inference on GPU** with configurable batch size.\r\n",
    "5. **Save results as artifacts**:\r\n",
    "   - `artifacts/embeddings/esm_embeddings.npy`\r\n",
    "   - `artifacts/embeddings/metadata.csv`\r\n",
    "6. Ensure embeddings are reproducible and ready for downstream ML.\r\n",
    "\r\n",
    "### Output of this notebook\r\n",
    "\r\n",
    "After running this notebook, you will have:\r\n",
    "\r\n",
    "- `X` — dense embedding matrix of shape `(N_proteins, 1280)`\r\n",
    "- `metadata.csv` — protein IDs, family labels, organisms, lengths\r\n",
    "- These artifacts are consumed in:\r\n",
    "\r\n",
    "### → `04_train_and_eval.ipynb`\r\n",
    "for classifier training, evaluation, MLflow logging and baselines.\r\n",
    "\r\n",
    "This notebook includes **only embedding computation** — no ML models yet.\r\n",
    "from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e591c6-94ce-46e9-980e-8aa7f22f3d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\ML\\BioML\\ESM\n",
      "PROCESSED_DIR: D:\\ML\\BioML\\ESM\\data\\processed\n",
      "EMBEDDINGS_DIR: D:\\ML\\BioML\\ESM\\artifacts\\embeddings\n",
      "  uniprot_id                                       protein_name  \\\n",
      "0     O00444  Serine/threonine-protein kinase PLK4 (EC 2.7.1...   \n",
      "1     O00506  Serine/threonine-protein kinase 25 (EC 2.7.11....   \n",
      "2     O00746  Nucleoside diphosphate kinase, mitochondrial (...   \n",
      "3     O14757  Serine/threonine-protein kinase Chk1 (EC 2.7.1...   \n",
      "4     O15111  Inhibitor of nuclear factor kappa-B kinase sub...   \n",
      "\n",
      "               organism  length  \\\n",
      "0  Homo sapiens (Human)     970   \n",
      "1  Homo sapiens (Human)     426   \n",
      "2  Homo sapiens (Human)     187   \n",
      "3  Homo sapiens (Human)     476   \n",
      "4  Homo sapiens (Human)     745   \n",
      "\n",
      "                                            sequence  family  \n",
      "0  MATCIGEKIEDFKVGNLLGKGSFAGVYRAESIHTGLEVAIKMIDKK...  kinase  \n",
      "1  MAHLRGFANQHSRVDPEELFTKLDRIGKGSFGEVYKGIDNHTKEVV...  kinase  \n",
      "2  MGGLFWRSALRGLRCGPRAPGPSLLVRHGSGGPSWTRERTLVAVKP...  kinase  \n",
      "3  MAVPFVEDWDLVQTLGEGAYGEVQLAVNRVTEEAVAVKIVDMKRAV...  kinase  \n",
      "4  MERPPGLRPGAGGPWEMRERLGTGGFGNVCLYQHRELDLKIAIKSC...  kinase  \n",
      "(4264, 6)\n",
      "family\n",
      "kinase           500\n",
      "transporter      499\n",
      "ligase           495\n",
      "chaperone        490\n",
      "transcription    484\n",
      "hydrolase        445\n",
      "ion_channel      420\n",
      "receptor         418\n",
      "protease         356\n",
      "dna_binding      157\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import esm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import PROCESSED_DIR, EMB_DIR, MIN_SEQ_LEN, MAX_SEQ_LEN\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "print(\"EMBEDDINGS_DIR:\", EMB_DIR)\n",
    "\n",
    "csv_path = PROCESSED_DIR / \"protein_families_small_clean.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df[\"family\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037e36b6-4690-4b07-904b-762764ef6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Загружаем предобученную модель и алфавит\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "padding_idx = alphabet.padding_idx\n",
    "max_positions = model.embed_positions.num_embeddings \n",
    "max_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce4ada9-2dc8-416a-8539-ed18bc6613f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool_token_reprs(token_representations: torch.Tensor,\n",
    "                          tokens: torch.Tensor,\n",
    "                          padding_idx: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Усреднение по аминокислотам (без BOS/EOS, без pad).\n",
    "    \n",
    "    token_representations: (B, L, D)\n",
    "    tokens: (B, L)\n",
    "    return: (B, D)\n",
    "    \"\"\"\n",
    "    pooled = []\n",
    "    for i in range(tokens.size(0)):\n",
    "        row = token_representations[i]   # (L, D)\n",
    "        row_tokens = tokens[i]           # (L,)\n",
    "\n",
    "        non_pad_idx = (row_tokens != padding_idx).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(non_pad_idx) <= 2:\n",
    "            vec = row[non_pad_idx].mean(dim=0)\n",
    "        else:\n",
    "            # [BOS] seq... [EOS]\n",
    "            start = non_pad_idx[1]        # пропускаем BOS\n",
    "            end = non_pad_idx[-2] + 1     # пропускаем EOS (slice [start:end)\n",
    "            vec = row[start:end].mean(dim=0)\n",
    "\n",
    "        pooled.append(vec)\n",
    "\n",
    "    return torch.stack(pooled, dim=0)    # (B, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612afe58-687f-4301-875c-c57e2ffd14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length in df_short: 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding sequences: 100%|██████████| 1066/1066 [13:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4264, 1280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df[\"uniprot_id\"].tolist()\n",
    "seqs = df[\"sequence\"].tolist()\n",
    "\n",
    "max_len_seq = max(len(s) for s in seqs)\n",
    "print(f\"Max sequence length in df_short: {max_len_seq}\")\n",
    "\n",
    "batch_size = 4  \n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(df_short), batch_size), desc=\"Embedding sequences\"):\n",
    "    batch_ids = ids[i:i + batch_size]\n",
    "    batch_seqs = seqs[i:i + batch_size]\n",
    "    batch_data = list(zip(batch_ids, batch_seqs))\n",
    "\n",
    "    labels, strs, tokens = batch_converter(batch_data)\n",
    "    # tokens.shape -> (B, L)\n",
    "    if tokens.size(1) > max_positions:\n",
    "        # защита от переполнения контекста\n",
    "        raise ValueError(\n",
    "            f\"Token sequence length {tokens.size(1)} exceeds model max_positions {max_positions}\"\n",
    "        )\n",
    "\n",
    "    tokens = tokens.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_reprs = out[\"representations\"][33]  # (B, L, D)\n",
    "\n",
    "    pooled = mean_pool_token_reprs(token_reprs, tokens, padding_idx)  # (B, D)\n",
    "    all_embeddings.append(pooled.cpu().numpy())\n",
    "\n",
    "# Собираем всё в одну матрицу\n",
    "X = np.vstack(all_embeddings)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e95b196-799a-46df-a079-47f08ba89a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('D:/ML/BioML/ESM/artifacts/embeddings/esm1b_embeddings_small_maxlen1000.npy'),\n",
       " WindowsPath('D:/ML/BioML/ESM/artifacts/embeddings/metadata_small_maxlen1000.csv'),\n",
       " (4264, 1280))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Убедимся, что размерности согласованы\n",
    "assert X.shape[0] == len(df_short), \"Mismatch between embeddings and dataframe length\"\n",
    "\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emb_path = EMB_DIR / \"esm1b_embeddings_small_maxlen1000.npy\"\n",
    "meta_path = EMB_DIR / \"metadata_small_maxlen1000.csv\"\n",
    "\n",
    "np.save(emb_path, X)\n",
    "\n",
    "# Сохраняем метаинформацию — чтобы потом удобно связывать с эмбеддингами\n",
    "meta_cols = [\"uniprot_id\", \"protein_name\", \"organism\", \"length\", \"family\"]\n",
    "df_short[meta_cols].to_csv(meta_path, index=False)\n",
    "\n",
    "emb_path, meta_path, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5063f17e-3d52-4bb7-9904-4d96ad881f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4264, 1280),\n",
       " (4264, 5),\n",
       " family\n",
       " kinase           500\n",
       " transporter      499\n",
       " ligase           495\n",
       " chaperone        490\n",
       " transcription    484\n",
       " hydrolase        445\n",
       " ion_channel      420\n",
       " receptor         418\n",
       " protease         356\n",
       " dna_binding      157\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Быстрая проверка, что всё читается\n",
    "X_loaded = np.load(emb_path)\n",
    "meta_loaded = pd.read_csv(meta_path)\n",
    "\n",
    "X_loaded.shape, meta_loaded.shape, meta_loaded[\"family\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b453b9-b914-48a2-b5c9-08cd45a1028f",
   "metadata": {},
   "source": [
    "## Summary of this notebook\r\n",
    "\r\n",
    "In this notebook we:\r\n",
    "\r\n",
    "1. **Loaded the cleaned protein family dataset**\r\n",
    "   - Source: `data/processed/protein_families_small_clean.csv`\r\n",
    "   - Final size: `N` proteins across 10 functional families  \r\n",
    "     (`kinase`, `transporter`, `ligase`, `chaperone`, `transcription`,\r\n",
    "     `hydrolase`, `ion_channel`, `receptor`, `protease`, `dna_binding`).\r\n",
    "\r\n",
    "2. **Loaded the ESM-1b model**\r\n",
    "   - Model: `esm1b_t33_650M_UR50S` (from `fair-esm`)\r\n",
    "   - Device: GPU (if available) / CPU fallback\r\n",
    "   - Representation layer: **33**\r\n",
    "   - Context limit respected by filtering sequences with  \r\n",
    "     `MIN_SEQ_LEN ≤ length ≤ MAX_SEQ_LEN` (here: `50–1000 aa`).\r\n",
    "\r\n",
    "3. **Computed sequence-level embeddings**\r\n",
    "   - Tokenized sequences with the ESM alphabet and batch converter.\r\n",
    "   - Extracted per-token representations from layer 33.\r\n",
    "   - Applied **mean pooling over amino acids**, excluding:\r\n",
    "     - padding tokens,\r\n",
    "     - BOS/EOS special tokens.\r\n",
    "   - Obtained a dense embedding matrix:\r\n",
    "     - shape: `(N_proteins, 1280)`.\r\n",
    "\r\n",
    "4. **Saved reusable artifacts**\r\n",
    "   - Embeddings:\r\n",
    "     - `artifacts/embeddings/esm1b_embeddings_small_maxlen1000.npy`\r\n",
    "   - Metadata (linked 1:1 to embeddings):\r\n",
    "     - `artifacts/embeddings/metadata_small_maxlen1000.csv`\r\n",
    "     - columns: `uniprot_id, protein_name, organism, length, family`\r\n",
    "\r\n",
    "These artifacts are now the **fixed input** for downstream ML steps:\r\n",
    "train/validation/test splits, model training, evaluation and interpretation.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Next step\r\n",
    "\r\n",
    "Proceed to:\r\n",
    "\r\n",
    "- `04_train_and_eval.ipynb`  \r\n",
    "\r\n",
    "where we will:\r\n",
    "\r\n",
    "- build train/val/test splits,\r\n",
    "- train baseline and stronger classifiers on top of ESM embeddings,\r\n",
    "- log results to MLflow for reproducibility.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esm_env)",
   "language": "python",
   "name": "esm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
